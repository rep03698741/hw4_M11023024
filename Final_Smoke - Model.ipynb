{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cs040\\anaconda3\\envs\\LAB_work\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\cs040\\anaconda3\\envs\\LAB_work\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\cs040\\anaconda3\\envs\\LAB_work\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR,SVC\n",
    "from math import sqrt\n",
    "from sklearn.tree import DecisionTreeClassifier ,DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor,GradientBoostingClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error,log_loss,accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from  sklearn.datasets  import  make_hastie_10_2\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X01</th>\n",
       "      <th>X02</th>\n",
       "      <th>X03</th>\n",
       "      <th>X04</th>\n",
       "      <th>X05</th>\n",
       "      <th>X06</th>\n",
       "      <th>X07</th>\n",
       "      <th>X08</th>\n",
       "      <th>X09</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>X24</th>\n",
       "      <th>Y</th>\n",
       "      <th>DI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.83</td>\n",
       "      <td>124.94</td>\n",
       "      <td>263.22</td>\n",
       "      <td>300.57</td>\n",
       "      <td>10.35</td>\n",
       "      <td>10.39</td>\n",
       "      <td>10.32</td>\n",
       "      <td>90.28</td>\n",
       "      <td>261.76</td>\n",
       "      <td>26.72</td>\n",
       "      <td>38.84</td>\n",
       "      <td>955.20</td>\n",
       "      <td>5715.00</td>\n",
       "      <td>5407.50</td>\n",
       "      <td>6407.50</td>\n",
       "      <td>7.37</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3776.31</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10.35</td>\n",
       "      <td>7.93</td>\n",
       "      <td>78.01</td>\n",
       "      <td>358689.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.97</td>\n",
       "      <td>125.30</td>\n",
       "      <td>262.12</td>\n",
       "      <td>300.53</td>\n",
       "      <td>10.33</td>\n",
       "      <td>10.40</td>\n",
       "      <td>10.34</td>\n",
       "      <td>89.50</td>\n",
       "      <td>268.51</td>\n",
       "      <td>26.84</td>\n",
       "      <td>37.54</td>\n",
       "      <td>961.50</td>\n",
       "      <td>5702.50</td>\n",
       "      <td>5407.50</td>\n",
       "      <td>6417.50</td>\n",
       "      <td>7.40</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3760.59</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.67</td>\n",
       "      <td>7.97</td>\n",
       "      <td>78.08</td>\n",
       "      <td>339139.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.90</td>\n",
       "      <td>125.00</td>\n",
       "      <td>261.45</td>\n",
       "      <td>300.29</td>\n",
       "      <td>10.35</td>\n",
       "      <td>10.41</td>\n",
       "      <td>10.33</td>\n",
       "      <td>89.74</td>\n",
       "      <td>263.78</td>\n",
       "      <td>25.93</td>\n",
       "      <td>34.98</td>\n",
       "      <td>952.19</td>\n",
       "      <td>5690.00</td>\n",
       "      <td>5392.50</td>\n",
       "      <td>6405.00</td>\n",
       "      <td>7.44</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3754.59</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.68</td>\n",
       "      <td>7.98</td>\n",
       "      <td>79.50</td>\n",
       "      <td>326082.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.33</td>\n",
       "      <td>125.25</td>\n",
       "      <td>262.12</td>\n",
       "      <td>298.94</td>\n",
       "      <td>10.31</td>\n",
       "      <td>10.41</td>\n",
       "      <td>10.33</td>\n",
       "      <td>89.69</td>\n",
       "      <td>260.09</td>\n",
       "      <td>27.34</td>\n",
       "      <td>37.11</td>\n",
       "      <td>937.14</td>\n",
       "      <td>5680.00</td>\n",
       "      <td>5392.50</td>\n",
       "      <td>6427.50</td>\n",
       "      <td>7.47</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3746.73</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.28</td>\n",
       "      <td>10.92</td>\n",
       "      <td>7.95</td>\n",
       "      <td>80.07</td>\n",
       "      <td>321295.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.37</td>\n",
       "      <td>125.03</td>\n",
       "      <td>262.94</td>\n",
       "      <td>301.73</td>\n",
       "      <td>10.33</td>\n",
       "      <td>10.40</td>\n",
       "      <td>10.32</td>\n",
       "      <td>90.20</td>\n",
       "      <td>262.26</td>\n",
       "      <td>41.93</td>\n",
       "      <td>45.88</td>\n",
       "      <td>921.01</td>\n",
       "      <td>5670.00</td>\n",
       "      <td>5377.50</td>\n",
       "      <td>6400.00</td>\n",
       "      <td>7.50</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3748.58</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.60</td>\n",
       "      <td>11.12</td>\n",
       "      <td>7.94</td>\n",
       "      <td>79.96</td>\n",
       "      <td>339248.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20540</th>\n",
       "      <td>40.66</td>\n",
       "      <td>124.73</td>\n",
       "      <td>262.06</td>\n",
       "      <td>300.62</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.53</td>\n",
       "      <td>11.11</td>\n",
       "      <td>154.49</td>\n",
       "      <td>765.99</td>\n",
       "      <td>37.04</td>\n",
       "      <td>45.45</td>\n",
       "      <td>945.13</td>\n",
       "      <td>5735.00</td>\n",
       "      <td>5358.50</td>\n",
       "      <td>5911.38</td>\n",
       "      <td>8.96</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4910.45</td>\n",
       "      <td>9.0</td>\n",
       "      <td>632.11</td>\n",
       "      <td>344.40</td>\n",
       "      <td>7.70</td>\n",
       "      <td>77.49</td>\n",
       "      <td>291896.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20541</th>\n",
       "      <td>43.77</td>\n",
       "      <td>126.15</td>\n",
       "      <td>267.76</td>\n",
       "      <td>304.75</td>\n",
       "      <td>10.64</td>\n",
       "      <td>10.57</td>\n",
       "      <td>11.06</td>\n",
       "      <td>157.19</td>\n",
       "      <td>768.79</td>\n",
       "      <td>37.10</td>\n",
       "      <td>45.14</td>\n",
       "      <td>952.87</td>\n",
       "      <td>5717.38</td>\n",
       "      <td>5345.63</td>\n",
       "      <td>5887.13</td>\n",
       "      <td>8.97</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4903.61</td>\n",
       "      <td>9.0</td>\n",
       "      <td>632.89</td>\n",
       "      <td>347.96</td>\n",
       "      <td>7.67</td>\n",
       "      <td>77.98</td>\n",
       "      <td>289563.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20542</th>\n",
       "      <td>41.27</td>\n",
       "      <td>124.20</td>\n",
       "      <td>262.99</td>\n",
       "      <td>300.61</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.57</td>\n",
       "      <td>11.03</td>\n",
       "      <td>157.85</td>\n",
       "      <td>748.09</td>\n",
       "      <td>36.67</td>\n",
       "      <td>45.09</td>\n",
       "      <td>949.86</td>\n",
       "      <td>5722.13</td>\n",
       "      <td>5341.75</td>\n",
       "      <td>5898.88</td>\n",
       "      <td>8.88</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4898.89</td>\n",
       "      <td>9.0</td>\n",
       "      <td>628.78</td>\n",
       "      <td>355.65</td>\n",
       "      <td>7.66</td>\n",
       "      <td>78.58</td>\n",
       "      <td>293656.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20543</th>\n",
       "      <td>41.39</td>\n",
       "      <td>124.99</td>\n",
       "      <td>262.99</td>\n",
       "      <td>298.91</td>\n",
       "      <td>10.68</td>\n",
       "      <td>10.59</td>\n",
       "      <td>11.05</td>\n",
       "      <td>156.53</td>\n",
       "      <td>749.15</td>\n",
       "      <td>32.92</td>\n",
       "      <td>42.97</td>\n",
       "      <td>957.37</td>\n",
       "      <td>5725.50</td>\n",
       "      <td>5357.50</td>\n",
       "      <td>5913.63</td>\n",
       "      <td>8.89</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4899.63</td>\n",
       "      <td>9.0</td>\n",
       "      <td>629.43</td>\n",
       "      <td>355.90</td>\n",
       "      <td>7.67</td>\n",
       "      <td>78.62</td>\n",
       "      <td>293181.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20544</th>\n",
       "      <td>42.85</td>\n",
       "      <td>125.39</td>\n",
       "      <td>266.16</td>\n",
       "      <td>302.25</td>\n",
       "      <td>10.66</td>\n",
       "      <td>10.56</td>\n",
       "      <td>11.08</td>\n",
       "      <td>154.45</td>\n",
       "      <td>763.38</td>\n",
       "      <td>39.09</td>\n",
       "      <td>45.98</td>\n",
       "      <td>937.53</td>\n",
       "      <td>5720.75</td>\n",
       "      <td>5340.00</td>\n",
       "      <td>5900.00</td>\n",
       "      <td>8.83</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4888.82</td>\n",
       "      <td>9.0</td>\n",
       "      <td>631.65</td>\n",
       "      <td>279.31</td>\n",
       "      <td>7.65</td>\n",
       "      <td>79.01</td>\n",
       "      <td>292206.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20545 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X01     X02     X03     X04    X05    X06    X07     X08     X09  \\\n",
       "0      41.83  124.94  263.22  300.57  10.35  10.39  10.32   90.28  261.76   \n",
       "1      41.97  125.30  262.12  300.53  10.33  10.40  10.34   89.50  268.51   \n",
       "2      41.90  125.00  261.45  300.29  10.35  10.41  10.33   89.74  263.78   \n",
       "3      42.33  125.25  262.12  298.94  10.31  10.41  10.33   89.69  260.09   \n",
       "4      42.37  125.03  262.94  301.73  10.33  10.40  10.32   90.20  262.26   \n",
       "...      ...     ...     ...     ...    ...    ...    ...     ...     ...   \n",
       "20540  40.66  124.73  262.06  300.62  10.65  10.53  11.11  154.49  765.99   \n",
       "20541  43.77  126.15  267.76  304.75  10.64  10.57  11.06  157.19  768.79   \n",
       "20542  41.27  124.20  262.99  300.61  10.65  10.57  11.03  157.85  748.09   \n",
       "20543  41.39  124.99  262.99  298.91  10.68  10.59  11.05  156.53  749.15   \n",
       "20544  42.85  125.39  266.16  302.25  10.66  10.56  11.08  154.45  763.38   \n",
       "\n",
       "         X10    X11     X12      X13      X14      X15   X16   X17      X18  \\\n",
       "0      26.72  38.84  955.20  5715.00  5407.50  6407.50  7.37  18.0  3776.31   \n",
       "1      26.84  37.54  961.50  5702.50  5407.50  6417.50  7.40  18.0  3760.59   \n",
       "2      25.93  34.98  952.19  5690.00  5392.50  6405.00  7.44  18.0  3754.59   \n",
       "3      27.34  37.11  937.14  5680.00  5392.50  6427.50  7.47  18.0  3746.73   \n",
       "4      41.93  45.88  921.01  5670.00  5377.50  6400.00  7.50  18.0  3748.58   \n",
       "...      ...    ...     ...      ...      ...      ...   ...   ...      ...   \n",
       "20540  37.04  45.45  945.13  5735.00  5358.50  5911.38  8.96  32.0  4910.45   \n",
       "20541  37.10  45.14  952.87  5717.38  5345.63  5887.13  8.97  32.0  4903.61   \n",
       "20542  36.67  45.09  949.86  5722.13  5341.75  5898.88  8.88  32.0  4898.89   \n",
       "20543  32.92  42.97  957.37  5725.50  5357.50  5913.63  8.89  32.0  4899.63   \n",
       "20544  39.09  45.98  937.53  5720.75  5340.00  5900.00  8.83  32.0  4888.82   \n",
       "\n",
       "        X19     X20     X21   X22    X23       X24  Y  DI  \n",
       "0      24.0    0.05   10.35  7.93  78.01  358689.0  3   0  \n",
       "1      24.0    0.00    9.67  7.97  78.08  339139.0  3   6  \n",
       "2      24.0    0.00    9.68  7.98  79.50  326082.0  3   6  \n",
       "3       8.0    5.28   10.92  7.95  80.07  321295.0  3   6  \n",
       "4       8.0    4.60   11.12  7.94  79.96  339248.0  2   6  \n",
       "...     ...     ...     ...   ...    ...       ... ..  ..  \n",
       "20540   9.0  632.11  344.40  7.70  77.49  291896.0  2   6  \n",
       "20541   9.0  632.89  347.96  7.67  77.98  289563.0  2   6  \n",
       "20542   9.0  628.78  355.65  7.66  78.58  293656.0  2   6  \n",
       "20543   9.0  629.43  355.90  7.67  78.62  293181.0  2   6  \n",
       "20544   9.0  631.65  279.31  7.65  79.01  292206.0  3   6  \n",
       "\n",
       "[20545 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始資料\n",
    "pd.set_option('display.max_columns',30)\n",
    "pd.set_option('display.max_rows',50)\n",
    "pd.set_option('max_colwidth',100)\n",
    "\n",
    "#給定columns\n",
    "train_columns = ['ID','TS','X01','X02','X03','X04','X05',\n",
    "               'X06','X07','X08','X09','X10','X11','X12'\n",
    "               ,'X13','X14','X15','X16','X17','X18','X19'\n",
    "                  ,'X20','X21','X22','X23','X24','Y','DI']\n",
    "test_colums = ['ID','TS','X01','X02','X03','X04','X05',\n",
    "               'X06','X07','X08','X09','X10','X11','X12'\n",
    "               ,'X13','X14','X15','X16','X17','X18','X19'\n",
    "                  ,'X20','X21','X22','X23','X24','DI']\n",
    "\n",
    "\n",
    "df_train_set = pd.read_csv('fgd_data/train_20545.csv', names=train_columns,skiprows=1,index_col=0)#na_values=' ?'\n",
    "df_test_set = pd.read_csv('fgd_data/Test_fin.csv', names=test_colums,skiprows=1,index_col=0)#skiprows=1, na_values=' ?'\n",
    "\n",
    "#刪除不必要欄位\n",
    "drop_name=['ID','TS']\n",
    "df_train_set.drop(drop_name, axis=1, inplace=True)\n",
    "df_test_set.drop(drop_name, axis=1, inplace=True)\n",
    "df_train_set = df_train_set.dropna()\n",
    "df_test_set = df_test_set.dropna()\n",
    "\n",
    "\n",
    "new_cols_train=list(df_train_set.columns)\n",
    "new_cols_train.remove('Y')\n",
    "\n",
    "#切分資料集\n",
    "x_train, y_train = df_train_set[new_cols_train].values, df_train_set['Y'].values\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.2,random_state=42)#random_state=42\n",
    "#x_train, x_validation, y_train, y_validation = df_train_set.iloc[:20144].values,df_train_set.iloc[20144:].values,df_train_set['Y'].iloc[:20144].values,df_train_set['Y'].iloc[20144:].values\n",
    "\n",
    "x_test = df_test_set.values\n",
    "\n",
    "df_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7340625327925455\n"
     ]
    }
   ],
   "source": [
    "#模型訓練、調參\n",
    "xgb = XGBClassifier(n_estimators=90, learning_rate=0.05 ,max_depth=19,eval_metric='mlogloss',subsample=0.9)\n",
    "xgb_fit = xgb.fit(x_train, y_train)\n",
    "y_pred = xgb_fit.predict_proba(x_validation)\n",
    "# xgb_fit = xgb.fit(x_train, y_train)\n",
    "# y_pred = xgb_fit.predict_proba(x_test)\n",
    "print(log_loss(y_validation,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [07:07<00:00, 21.38s/it]\n"
     ]
    }
   ],
   "source": [
    "#xgb\n",
    "#將預測績效儲存csv\n",
    "xgb = XGBClassifier(n_estimators=90, learning_rate= 0.05,max_depth=15,eval_metric='mlogloss')\n",
    "xgb_fit = xgb.fit(x_train, y_train)\n",
    "y_pred = xgb_fit.predict_proba(x_test)\n",
    "df_pre = pd.DataFrame(y_pred)\n",
    "df_submit = pd.read_csv('./fgd_data/submit_samples.csv')\n",
    "df_submit = df_submit[['ID']]\n",
    "df_submit = df_submit.join(df_pre) \n",
    "df_submit.columns = ['ID', 'C1', 'C2', 'C3', 'C4', 'C5']\n",
    "df_submit\n",
    "\n",
    "df_submit.to_csv('C:/Users/cs040/anaconda3/envs/LAB_work/Scripts/fgd_data/xgb_'+str(i)+'.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest\n",
    "#將預測績效儲存csv\n",
    "clf= RandomForestClassifier(random_state=42,max_depth=18,n_estimators=150,criterion='entropy')#160 180 0.78\n",
    "R_fit = clf.fit(x_train, y_train)\n",
    "y_pred = R_fit.predict_proba(x_test)\n",
    "\n",
    "df_pre = pd.DataFrame(y_pred)\n",
    "df_submit = pd.read_csv('./fgd_data/submit_samples.csv')\n",
    "df_submit = df_submit[['ID']]\n",
    "df_submit = df_submit.join(df_pre) \n",
    "df_submit.columns = ['ID', 'C1', 'C2', 'C3', 'C4', 'C5']\n",
    "df_submit\n",
    "\n",
    "df_submit.to_csv('C:/Users/cs040/anaconda3/envs/LAB_work/Scripts/fgd_data/xgb_'+str(i)+'.csv',index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
